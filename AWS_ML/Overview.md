**AI** 	- Theory and methods to build machines that think and act like humans.  
**ML** 	- is a subfield of AI, that trains a model from input data.  
**DL** 	- Type of machine learning that uses artificial neural networks, allowing them to process more complex patterns than traditional ML.  
**Gen AI**	- subset of DL which uses artificial neural networks and process labeled and non-labeled data using supervised, unsupervised or semi-supervised methods.  
**LLM**	- Subset of DL which refers to large, general purpose language models that can be pre-trained and then fine-tuned for specific purposes.
<br> 

**ML** 	- The trained model can make useful predictions from new or never before seen data drawn from the same one used to train the model.
  - Gives the computer the ability to learn without explicit programming
  - Common classes of ML models are: Unsupervised and Supervised (data has labels or tags)

**Supervised learning** - the model learns from past examples to predict future values
  - Input: testing data values, model outputs a prediction and compares the prediction to the training data used to train the model, if both values are far apart then its an error and model tries to reduce the error until both values are close together.

**Unsupervised learning** - the problems are about discovery, about looking at raw data and seeing if it naturally falls into groups
  - Generalize the data to new examples.  
<br>

**Deep Learning** 	- The neurons in the neural networks are interconnected and can learn to perform tasks by processing data and making predictions. 
- Deep Learning models typically have many layers of neurons which allows them to learn more complex patterns than traditional ML.
- Can use both labeled and non-labeled data. Called semi-supervised learning.
- DL models can be divided into generative and discriminative
- So if we just had one layer there, that would be a **deep neural network**. But when you have multiple layers,that's where we talk about deep learning.

**Discriminative model** - Used to classify or predict labels for data points
- Typically trained on a dataset of labeled data
- Learns the relationship between the features of the data point and the labels.
- Learns conditional probability distribution or probability of y given an input x. Ex if input x is an image of dog then y the output will be the classification of x as dog.

**Generative model** - Generates new data instances based on learned probability distribution of existing data or the data it was trained on.
- Generates new content by understanding the distribution of data
- Learns the Joint probability distribution or the probability of x and y and predicts the conditional probability and then generate new data. Ex if input is an image of dog then the conditional probability of dog is arrived and a new picture of a dog is generated.

A good way to determine if the output from the ML system (called y or label), is gen AI or not is to look is :
  - Its Not Gen AI if the output is number, discrete, class(ex spam or not spam) or probability 
  - Its Gen AI if output is Natural language (ex speech or text), image or audio

**<u>What is Generative AI</u>**
- GenAI is a type of AI that creates new content based on what it has learned from existing content
- The process of learning from existing content is called training and results in the creation of statistical model
- When given a prompt, GenAI uses this statistical model to predict what an expected response might be - and this generates new content.
- It learns the underlying structure of the data and can then generate new samples that are similar to the data it was trained on.

**Generative Language Models** - Learn about patterns in a language through training data. Then given some text, they predict what comes next.
- Can take input as text and then output Text (Translation, Summarization, Question answering, grammar correction), Image (Image generation, Video generation), Audio (text to speech), Decisions (Play Games)

**Generative Image Models** - Produce new images using techniques like Diffusion. Then given a prompt or related imagery, they transform random noise into images or generate images from prompt.
- Can take an input as image and output Text (Image captioning, Visual question answering, Image search), another image (Super resolution, Image completion), Video (Animation)

**Transformers**
- Consists of encoder and decoder. Encoder encodes the input sequence and passes it to decoder which learns how to decode the representation for a relevant task
- Hallucinations are output generated by the model that are non sensical or grammatically incorrect. Caused by no. of factors like not trained on enough data, trained on noisy or dirty data, not given enough context, or not given enough constraints. 

**Prompt Design**
	- The quality of the input determines the quality of the output. 
	- Prompt design is the process of creating a prompt that will generate the desired output from a LLM.
	- A prompt is a short piece of text that is given to LLM as input and it can be used to control the output of the model in a variety of models.
**Prompt Engineering**
	- is the process of creating a prompt that is designed to improve performance. This may involve domain knowledge, providing examples of desired output, or using keywords that are known to be effective for the specific system.
	- prompt design is a more general concept while prompt engineering is a more specialized concept.

**Foundation Models** - is a large AI model pre-trained on a vast quantity of data designed to be adapted or fine tuned to a wide range of downstream tasks such as sentiment analysis, image captioning, object recognition etc.

**Vertex AI** - Offers a model garden that includes foundation models. The Language foundation models include PaLM API for chat and text, and BERT. The Vision foundation models includes Stable Diffusion (generates High quality images from text description), Embeddings extractor, BLIP image captioning, BLIP VQA, CLIP, OWL-ViT, ViT GPT2.

**Parameters** - defines the skill of a model in solving a problem, such as predicting text.
	- They are the memories and the knowledge that the machine learned from the model training

**"Few Shot"** refers to training a model with minimal data and "Zero shot" implies that a model can recognize things that have not explicitly been taught in the training before.

**LLM**	- 3 kinds: Generic LM, Instruction tuned, Dialog tuned.
- Generic Language Model predicts the next word or technically token based on the language in the training data.
- Token is part of a word, the atomic unit that LLMs work in.
- Instruction tuned, the model is trained to predict a response to the instruction given in the input. Ex classify the instruction/text into neutral, negative or positive.
- Dialog tuned models are special case of instruction tuned models where requests are typically framed as questions to a chat bot. 
Fine tuning - Bring you own dataset and retrain the model by training every weight in the LLM. 
- Fine tuning is expensive and not realistic in many cases.
Parameter-efficient tuning - methods for training a large language model on your own custom data without duplicating the model. The base model itself is not altered.
- Instead a small number of add-on layers are tuned, which can be swapped in and out at inference time. 

A **Poisson distribution** is a discrete probability distribution. It gives the probability of an event happening a certain number of times (k) within a given interval of time or space.
The Poisson distribution has only one parameter, Î» (lambda), which is the mean number of events. 